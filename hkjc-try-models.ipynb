{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Flatten(), nn.Linear(input_size, layers[0])])\n",
    "        for idx, size in enumerate(layers[:-1]):\n",
    "            self.layers.append(nn.Linear(size, layers[idx + 1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(layers[-1], 1))\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model (x_tensor, y_tensor, layers):\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    # print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "    in_path = os.path.join(\"drive\",\"MyDrive\",\"Colab Notebooks\",\"HKJC-ML\")\n",
    "\n",
    "    # x_tensor = torch.load(os.path.join(in_path, \"x_tensor\")).to(torch.float32).to(device)\n",
    "    # y_tensor = torch.load(os.path.join(in_path, \"y_tensor\")).to(torch.float32).to(device)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "    rng = torch.Generator().manual_seed(42)\n",
    "    training_set, val_set = torch.utils.data.random_split(dataset, [0.8, 0.2], generator=rng)\n",
    "\n",
    "    input_size = x_tensor.shape[1]\n",
    "    model = MLP(input_size, layers).to(device)\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 64\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    epochs = 5000\n",
    "    patience = 250\n",
    "\n",
    "    train_loss_plot = []\n",
    "    val_loss_plot = []\n",
    "    epochs_plot = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # if e % 10 == 0:\n",
    "        #     print(f\"Epoch {e}\\n-------------------------------\")\n",
    "        size = len(train_dataloader.dataset)\n",
    "        # Set the model to training mode - important for batch normalization and dropout layers\n",
    "        # Unnecessary in this situation but added for best practices\n",
    "        model.train()\n",
    "        train_loss_sum = 0\n",
    "        num_train_batches = 0\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            # Compute prediction and loss\n",
    "            # X = torch.swapaxes(X, 0, 2)\n",
    "            pred = model(X)\n",
    "            y = torch.unsqueeze(y, dim=1)\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_loss_sum += loss.item()\n",
    "            num_train_batches += 1\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        train_loss_plot.append(train_loss_sum/num_train_batches)\n",
    "\n",
    "        # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "        # Unnecessary in this situation but added for best practices\n",
    "        model.eval()\n",
    "        size = len(val_dataloader.dataset)\n",
    "        num_val_batches = len(val_dataloader)\n",
    "        val_loss, correct = 0, 0\n",
    "\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during val mode\n",
    "        # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_dataloader:\n",
    "                pred = model(X)\n",
    "                y = torch.unsqueeze(y, dim=1)\n",
    "                val_loss += loss_fn(pred, y).item()\n",
    "                # correct += (pred == y).type(torch.float).sum().item()\n",
    "\n",
    "        val_loss /= num_val_batches\n",
    "        val_loss_plot.append(val_loss)\n",
    "        epochs_plot.append(e+1)\n",
    "\n",
    "        # if e % 10 == 0:\n",
    "        # print(f\"val loss: {val_loss:>8f} \\n\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                break\n",
    "\n",
    "    # plt.plot(epochs_plot, train_loss_plot, label = \"training loss\")\n",
    "    # plt.plot(epochs_plot, val_loss_plot, label = \"validation loss\")\n",
    "    # plt.legend()\n",
    "    # print('best val loss', best_val_loss)\n",
    "\n",
    "    file_name = f\"{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{batch_size}_{e}_{str(f'{best_val_loss:.2g}').split('.')[-1]}\"\n",
    "\n",
    "    out_path = os.path.join(\"drive\",\"MyDrive\",\"Colab Notebooks\",\"HKJC-ML\", \"model_configs\", \"hkjc5\", file_name)\n",
    "    torch.save(best_model_state, out_path)\n",
    "\n",
    "    print(file_name, 'saved', 'best val loss', best_val_loss)\n",
    "\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_cols = ['total_stakes_rank','horse_weight_rank','horse_handicap_rank','horse_odds_rank','horse_rating_rank','days_since_import_rank',\n",
    "            'jockey_age_rank','jockey_rides_rank','jockey_stakes_rank','jockey_same_race_wins_rank']\n",
    "\n",
    "layers_to_try = [[64,32,16,8],[64,32,16,8,4],[32,64,32,16,8],[64,128,64,32,16,8],[64,128,256,128,64,32,16,8],\n",
    "            [64,128,256,512,256,128,64,32,16,8],[64,128,256,512,1024,256,128,64,32,16,8]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict = {'cols_kept':[],'layers':[],'file':[]}\n",
    "\n",
    "# df = pd.DataFrame(dict)\n",
    "# df.to_csv(os.path.join('data','5_ordinal_mean_tensor','model_names.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexlee/Documents/Coding/HKJC-ML/hkjc5/hkjc-try-models.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexlee/Documents/Coding/HKJC-ML/hkjc5/hkjc-try-models.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m entire_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexlee/Documents/Coding/HKJC-ML/hkjc5/hkjc-try-models.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_name \u001b[39min\u001b[39;00m [f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(in_path) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)]:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexlee/Documents/Coding/HKJC-ML/hkjc5/hkjc-try-models.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(in_path, file_name), index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexlee/Documents/Coding/HKJC-ML/hkjc5/hkjc-try-models.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     entire_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([entire_df, df])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexlee/Documents/Coding/HKJC-ML/hkjc5/hkjc-try-models.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model_names \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m5_ordinal_mean_tensor\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmodel_names.csv\u001b[39m\u001b[39m'\u001b[39m), index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py:1795\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1793\u001b[0m         new_rows \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(index)\n\u001b[0;32m-> 1795\u001b[0m     df \u001b[39m=\u001b[39m DataFrame(col_dict, columns\u001b[39m=\u001b[39;49mcolumns, index\u001b[39m=\u001b[39;49mindex)\n\u001b[1;32m   1797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_rows\n\u001b[1;32m   1799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqueeze \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(df\u001b[39m.\u001b[39mcolumns) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/internals/construction.py:443\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseries\u001b[39;00m \u001b[39mimport\u001b[39;00m Series\n\u001b[0;32m--> 443\u001b[0m     arrays \u001b[39m=\u001b[39m Series(data, index\u001b[39m=\u001b[39;49mcolumns, dtype\u001b[39m=\u001b[39;49m\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m    444\u001b[0m     missing \u001b[39m=\u001b[39m arrays\u001b[39m.\u001b[39misna()\n\u001b[1;32m    445\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# GH10856\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# raise ValueError if only scalars in dict\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/series.py:399\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[39m# uncomment the line below when removing the FutureWarning\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[39m# dtype = np.dtype(object)\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     data \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/indexes/base.py:7331\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7329\u001b[0m         \u001b[39mreturn\u001b[39;00m MultiIndex\u001b[39m.\u001b[39mfrom_arrays(index_like)\n\u001b[1;32m   7330\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 7331\u001b[0m         \u001b[39mreturn\u001b[39;00m Index\u001b[39m.\u001b[39;49m_with_infer(index_like, copy\u001b[39m=\u001b[39;49mcopy, tupleize_cols\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   7332\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   7333\u001b[0m     \u001b[39mreturn\u001b[39;00m Index\u001b[39m.\u001b[39m_with_infer(index_like, copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/indexes/base.py:715\u001b[0m, in \u001b[0;36mIndex._with_infer\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39mConstructor that uses the 1.0.x behavior inferring numeric dtypes\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mfor ndarray[object] inputs.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m--> 715\u001b[0m     warnings\u001b[39m.\u001b[39;49mfilterwarnings(\u001b[39m\"\u001b[39;49m\u001b[39mignore\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m.*the Index constructor\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFutureWarning\u001b[39;49;00m)\n\u001b[1;32m    716\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    718\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m _dtype_obj \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_is_multi:\n\u001b[1;32m    719\u001b[0m     \u001b[39m# error: Argument 1 to \"maybe_convert_objects\" has incompatible type\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# \"ndarray[Any, Any]\"\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/warnings.py:148\u001b[0m, in \u001b[0;36mfilterwarnings\u001b[0;34m(action, message, category, module, lineno, append)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39missubclass\u001b[39m(category, \u001b[39mWarning\u001b[39;00m), \u001b[39m\"\u001b[39m\u001b[39mcategory must be a Warning subclass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(module, \u001b[39mstr\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mmodule must be a string\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(lineno, \u001b[39mint\u001b[39;49m) \u001b[39mand\u001b[39;00m lineno \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \\\n\u001b[1;32m    149\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mlineno must be an int >= 0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m message \u001b[39mor\u001b[39;00m module:\n\u001b[1;32m    152\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_path = os.path.join('data','5_ordinal_mean_tensor','evaluation')\n",
    "\n",
    "entire_df = pd.DataFrame()\n",
    "\n",
    "for file_name in [f for f in os.listdir(in_path) if not f.startswith(\".\")]:\n",
    "    df = pd.read_csv(os.path.join(in_path, file_name), index_col=0)\n",
    "\n",
    "    entire_df = pd.concat([entire_df, df])\n",
    "\n",
    "model_names = pd.read_csv(os.path.join('data','5_ordinal_mean_tensor','model_names.csv'), index_col=0)\n",
    "\n",
    "for _ in range(50):\n",
    "    while True:\n",
    "        c = random.choice(rank_cols)\n",
    "        layers = random.choice(layers_to_try)\n",
    "\n",
    "        for idx, row in model_names.iterrows():\n",
    "            if row['cols_kept'] != c or row['layers'] != layers:\n",
    "                break\n",
    "\n",
    "    cols_to_drop = rank_cols.copy()\n",
    "    cols_to_drop.remove(c)\n",
    "    cols_to_drop.append(['race_index','place','finish_time'])\n",
    "\n",
    "    x_df = entire_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    # finish time in seconds\n",
    "    y_df = entire_df['finish_time']\n",
    "    x = x_df.to_numpy()\n",
    "    y = y_df.to_numpy()\n",
    "    x_tensor = torch.from_numpy(x)\n",
    "    y_tensor = torch.from_numpy(y)\n",
    "    # torch.save(x_tensor, os.path.join(out_path, 'finish_time', \"x_tensor\"))\n",
    "    # torch.save(y_tensor, os.path.join(out_path, 'finish_time', \"y_tensor\"))\n",
    "\n",
    "    model_config_name = train_model(x_tensor, y_tensor, layers)\n",
    "\n",
    "    model_names_dict = model_names.to_dict('list')\n",
    "    model_names_dict['cols_kept'].append(c)\n",
    "    model_names_dict['layers'].append(layers)\n",
    "    model_names_dict['file'].append(model_config_name)\n",
    "    pd.DataFrame(model_names_dict).to_csv(os.path.join('data','5_ordinal_mean_tensor','model_names.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
