{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pprint\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n",
      "<_PrefetchDataset element_spec={'bucketized_user_age': TensorSpec(shape=(), dtype=tf.float32, name=None), 'movie_genres': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'movie_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'movie_title': TensorSpec(shape=(), dtype=tf.string, name=None), 'raw_user_age': TensorSpec(shape=(), dtype=tf.float32, name=None), 'timestamp': TensorSpec(shape=(), dtype=tf.int64, name=None), 'user_gender': TensorSpec(shape=(), dtype=tf.bool, name=None), 'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_occupation_label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'user_occupation_text': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float32, name=None), 'user_zip_code': TensorSpec(shape=(), dtype=tf.string, name=None)}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 13:42:50.560822: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "\n",
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)\n",
    "\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec={'movie_title': TensorSpec(shape=(), dtype=tf.string, name=None), 'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'timestamp': TensorSpec(shape=(), dtype=tf.int64, name=None), 'user_rating': TensorSpec(shape=(), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('data','all_races_cleaned.csv'), index_col=0)\n",
    "\n",
    "ratings = pd.DataFrame()\n",
    "ratings['movie_title'] = df['horse_colour'].astype(str)\n",
    "ratings['user_id'] = df['race_class'].astype(str)\n",
    "ratings['timestamp'] = df['horse_weight'].astype('int64')\n",
    "ratings['user_rating'] = df['place'].astype('float32')\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings))\n",
    "# ratings = tf.cast(ratings, tf.string)\n",
    "\n",
    "# movies = pd.DataFrame()\n",
    "# movies['movie_title'] = df['horse_colour'].astype(str)\n",
    "# movies = tf.data.Dataset.from_tensor_slices(dict(movies))\n",
    "# movies = tf.cast(movies, tf.string)\n",
    "\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['[UNK]', 'bay', 'chestnut']\n",
      "tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 0.01168508 -0.02628584 -0.03255516  0.00796981  0.04420919 -0.00262135\n",
      "  -0.00652572  0.02385812 -0.01603056 -0.02144766 -0.03840078  0.00241581\n",
      "  -0.03845628 -0.01541411  0.02311036 -0.00094662  0.0314948  -0.00097756\n",
      "   0.02480873 -0.00167195 -0.0293787   0.03293855 -0.01756399  0.03900624\n",
      "   0.03613484  0.01605323 -0.04101385  0.02197155  0.01800734  0.00361491\n",
      "  -0.00247269  0.00419196]], shape=(1, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "movie_title_lookup = tf.keras.layers.StringLookup()\n",
    "movie_title_lookup.adapt(ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "print(f\"Vocabulary: {movie_title_lookup.get_vocabulary()[:3]}\")\n",
    "\n",
    "print(movie_title_lookup([\"Star Wars (1977)\", \"One Flew Over the Cuckoo's Nest (1975)\"]))\n",
    "\n",
    "movie_title_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=movie_title_lookup.vocabulary_size(),\n",
    "    output_dim=32\n",
    ")\n",
    "\n",
    "movie_title_model = tf.keras.Sequential([movie_title_lookup, movie_title_embedding])\n",
    "\n",
    "print(movie_title_model([\"Star Wars (1977)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([101016  96565], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# We set up a large number of bins to reduce the chance of hash collisions.\n",
    "num_hashing_bins = 200_000\n",
    "\n",
    "movie_title_hashing = tf.keras.layers.Hashing(\n",
    "    num_bins=num_hashing_bins\n",
    ")\n",
    "\n",
    "print(movie_title_hashing([\"Star Wars (1977)\", \"One Flew Over the Cuckoo's Nest (1975)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_lookup = tf.keras.layers.StringLookup()\n",
    "user_id_lookup.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(user_id_lookup.vocabulary_size(), 32)\n",
    "\n",
    "user_id_model = tf.keras.Sequential([user_id_lookup, user_id_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 1153.\n",
      "Timestamp: 1076.\n",
      "Timestamp: 1142.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Timestamp: {x['timestamp']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized timestamp: [0.39381862].\n",
      "Normalized timestamp: [-0.7245862].\n",
      "Normalized timestamp: [0.2340465].\n"
     ]
    }
   ],
   "source": [
    "timestamp_normalization = tf.keras.layers.Normalization(\n",
    "    axis=None\n",
    ")\n",
    "timestamp_normalization.adapt(ratings.map(lambda x: x[\"timestamp\"]).batch(1024))\n",
    "\n",
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Normalized timestamp: {timestamp_normalization(x['timestamp'])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: [930.         930.57357357 931.14714715]\n",
      "Timestamp embedding: [[-0.01126814  0.01927097  0.04903176  0.03438972 -0.04880816  0.00473027\n",
      "   0.00607485 -0.03134649  0.00976001  0.02342154  0.00758326 -0.00995881\n",
      "  -0.03468177  0.00965523 -0.01875994  0.02685263 -0.02455081  0.03758484\n",
      "   0.04908016 -0.01390896  0.040326   -0.01622738 -0.02180339 -0.00511694\n",
      "  -0.03684079  0.04526445  0.02674117  0.04600663  0.03111876  0.02676419\n",
      "   0.0324604   0.03121363]].\n"
     ]
    }
   ],
   "source": [
    "max_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
    "min_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    np.int64(1e9), tf.minimum).numpy().min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000)\n",
    "\n",
    "print(f\"Buckets: {timestamp_buckets[:3]}\")\n",
    "\n",
    "timestamp_embedding_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "    tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32)\n",
    "])\n",
    "\n",
    "for timestamp in ratings.take(1).map(lambda x: x[\"timestamp\"]).batch(1).as_numpy_iterator():\n",
    "    print(f\"Timestamp embedding: {timestamp_embedding_model(timestamp)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2]], shape=(1, 1), dtype=int64)\n",
      "['', '[UNK]', 'bay', 'chestnut', 'brown']\n"
     ]
    }
   ],
   "source": [
    "title_text = tf.keras.layers.TextVectorization()\n",
    "title_text.adapt(ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(1):\n",
    "    print(title_text(row))\n",
    "\n",
    "print(title_text.get_vocabulary()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        user_id_lookup,\n",
    "        tf.keras.layers.Embedding(user_id_lookup.vocabulary_size(), 32),\n",
    "    ])\n",
    "    self.timestamp_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "      tf.keras.layers.Embedding(len(timestamp_buckets) + 2, 32)\n",
    "    ])\n",
    "    self.normalized_timestamp = tf.keras.layers.Normalization(\n",
    "        axis=None\n",
    "    )\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    # Take the input dictionary, pass it through each input layer,\n",
    "    # and concatenate the result.\n",
    "    return tf.concat([\n",
    "        self.user_embedding(inputs[\"user_id\"]),\n",
    "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1))\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed representations: [-0.04870258 -0.01183021 -0.02222172]\n"
     ]
    }
   ],
   "source": [
    "user_model = UserModel()\n",
    "\n",
    "user_model.normalized_timestamp.adapt(\n",
    "    ratings.map(lambda x: x[\"timestamp\"]).batch(128))\n",
    "\n",
    "for row in ratings.batch(1).take(1):\n",
    "    print(f\"Computed representations: {user_model(row)[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    max_tokens = 10_000\n",
    "\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      movie_title_lookup,\n",
    "      tf.keras.layers.Embedding(movie_title_lookup.vocabulary_size(), 32)\n",
    "    ])\n",
    "    self.title_text_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.TextVectorization(max_tokens=max_tokens),\n",
    "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "      # We average the embedding of individual words to get one embedding vector\n",
    "      # per title.\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.concat([\n",
    "        self.title_embedding(inputs[\"movie_title\"]),\n",
    "        self.title_text_embedding(inputs[\"movie_title\"]),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed representations: [-0.03881745 -0.02605334 -0.0085703 ]\n"
     ]
    }
   ],
   "source": [
    "movie_model = MovieModel()\n",
    "\n",
    "movie_model.title_text_embedding.layers[0].adapt(\n",
    "    ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "for row in ratings.batch(1).take(1):\n",
    "  print(f\"Computed representations: {movie_model(row)[0, :3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hkjc-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
