{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_03_13_41_0.0001_64_552_0.0053\n",
    "\n",
    "2023_11_06_21_30_0.001_64_367_0052 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_03_14_24_0.0001_64_624_0.0052\n",
    "\n",
    "2023_11_06_13_25_0.001_64_331_0053 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_03_23_19_0.001_64_388_0.0053\n",
    "\n",
    "2023_11_05_21_22_0.001_64_382_0052 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_03_23_41_0.001_64_369_0.0052\n",
    "\n",
    "2023_11_06_21_41_0.001_64_375_0054 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_04_12_36_0.001_64_380_0.0051\n",
    "\n",
    "2023_11_05_21_48_0.001_64_337_0052 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 1024),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(1024, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_04_12_54_0.001_64_449_0.0051\n",
    "\n",
    "2023_11_05_22_07_0.001_64_331_0052 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, 2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(2, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_04_13_25_0.001_64_374_0.0052\n",
    "\n",
    "no 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, 2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(2, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_04_13_43_0.001_64_347_0.0052\n",
    "\n",
    "2023_11_08_19_20_64_384_0053 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 1024),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(1024, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, 2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(2, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_04_14_00_0.001_64_376_0.0057\n",
    "\n",
    "no 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 1024),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(1024, 1024),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(1024, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_04_14_18_0.001_64_426_0.0052\n",
    "\n",
    "2023_11_04_16_03_0.001_32_322_005\n",
    "\n",
    "2023_11_05_21_34_0.001_64_328_0051 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best\n",
    "\n",
    "2023_11_04_14_49_0.001_64_365_0.0052\n",
    "\n",
    "2023_11_08_18_56_64_377_0053 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_09_20_18_64_379_0052 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, 2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(2, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_09_20_32_64_302_0052 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, 2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(2, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_09_20_03_64_338_0051 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_09_19_46_64_343_0051 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_09_19_17_64_350_0051 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 25),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(25, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_09_18_59_64_284_0055 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_09_18_51_64_299_0053 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_04_15_16_0.001_64_644_0052\n",
    "\n",
    "2023_11_08_19_07_64_357_0052 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, 4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(4, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023_11_04_15_41_0.001_64_323_0051\n",
    "\n",
    "2023_11_05_21_58_0.001_64_348_0051 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(input_size, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 16),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(16, 8),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(8, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
